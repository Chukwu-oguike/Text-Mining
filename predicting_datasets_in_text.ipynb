{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58xFYxkQ5BPE"
      },
      "source": [
        "\n",
        "This project is aim at finding within-text data citations in scientific research articles. The results from this project can show how public data are being used in science and help the policy makers make wiser, more transparent public investments. It will help move researchers and governments from using ad-hoc methods to automated ways of finding out what datasets are being used to solve problems, what measures are being generated, and which researchers are the experts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmxabIkp5BPG"
      },
      "outputs": [],
      "source": [
        "# from zipfile import ZipFile as zp\n",
        "import re\n",
        "# import os\n",
        "# from nltk.corpus import stopwords\n",
        "# from nltk.stem.porter import PorterStemmer\n",
        "# import nltk\n",
        "# import spacy\n",
        "# from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "# from unidecode import unidecode\n",
        "import numpy as np\n",
        "# from urllib.request import Request, urlopen\n",
        "# import requests\n",
        "# import bs4\n",
        "import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "# import json\n",
        "# import itertools\n",
        "# from gensim.models import Word2Vec\n",
        "# from nltk.corpus import wordnet as wn\n",
        "# from numpy import array\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense\n",
        "# from keras.layers import Dropout\n",
        "\n",
        "# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "# from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IR1OCZs5BPH",
        "outputId": "0d61b09d-3318-4ed1-a4f6-69e243bb3bc5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /cluster/home/coguik01/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /cluster/home/coguik01/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words = set(list(stop_words))\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "let_me_tize_u = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pqY80_a5BPI"
      },
      "source": [
        "#### Unzip Training Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doR2-smB5BPI"
      },
      "outputs": [],
      "source": [
        "with zp('/cluster/tufts/data/train.csv.zip') as zipObj:\n",
        "    zipObj.extractall('/cluster/tufts//data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D-Btgu25BPJ"
      },
      "source": [
        "#### Retrieve Data Rubric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhqE5XPx5BPJ",
        "outputId": "f0dc25b8-850c-4e77-8813-0e1c0f7649a5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>pub_title</th>\n",
              "      <th>dataset_title</th>\n",
              "      <th>dataset_label</th>\n",
              "      <th>cleaned_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>d0fa7568-7d8e-4db9-870f-f9c6f668c17b</td>\n",
              "      <td>The Impact of Dual Enrollment on College Degre...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>national education longitudinal study</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2f26f645-3dec-485d-b68d-f013c9e05e60</td>\n",
              "      <td>Educational Attainment of High School Dropouts...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>national education longitudinal study</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>c5d5cd2c-59de-4f29-bbb1-6a88c7b52f29</td>\n",
              "      <td>Differences in Outcomes for Female and Male St...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>national education longitudinal study</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5c9a3bc9-41ba-4574-ad71-e25c1442c8af</td>\n",
              "      <td>Stepping Stone and Option Value in a Model of ...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>national education longitudinal study</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>c754dec7-c5a3-4337-9892-c02158475064</td>\n",
              "      <td>Parental Effort, School Resources, and Student...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>national education longitudinal study</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14311</th>\n",
              "      <td>f89dd9fa-07af-4384-aa0c-0d14602c0cea</td>\n",
              "      <td>Artificial Intelligence of COVID-19 Imaging: A...</td>\n",
              "      <td>RSNA International COVID-19 Open Radiology Dat...</td>\n",
              "      <td>RSNA International COVID-19 Open Radiology Dat...</td>\n",
              "      <td>rsna international covid 19 open radiology dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14312</th>\n",
              "      <td>b3498176-8832-4033-aea6-b5ea85ea04c4</td>\n",
              "      <td>RSNA International Trends: A Global Perspectiv...</td>\n",
              "      <td>RSNA International COVID-19 Open Radiology Dat...</td>\n",
              "      <td>RSNA International COVID Open Radiology Database</td>\n",
              "      <td>rsna international covid open radiology database</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14313</th>\n",
              "      <td>f77eb51f-c3ac-420b-9586-cb187849c321</td>\n",
              "      <td>MCCS: a novel recognition pattern-based method...</td>\n",
              "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
              "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
              "      <td>cas covid 19 antiviral candidate compounds dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14314</th>\n",
              "      <td>ab59bcdd-7b7c-4107-93f5-0ccaf749236c</td>\n",
              "      <td>Quantitative Structure–Activity Relationship M...</td>\n",
              "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
              "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
              "      <td>cas covid 19 antiviral candidate compounds dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14315</th>\n",
              "      <td>fd23e7e0-a5d2-4f98-992d-9209c85153bb</td>\n",
              "      <td>A ligand-based computational drug repurposing ...</td>\n",
              "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
              "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
              "      <td>cas covid 19 antiviral candidate compounds dat...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14316 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         Id  \\\n",
              "0      d0fa7568-7d8e-4db9-870f-f9c6f668c17b   \n",
              "1      2f26f645-3dec-485d-b68d-f013c9e05e60   \n",
              "2      c5d5cd2c-59de-4f29-bbb1-6a88c7b52f29   \n",
              "3      5c9a3bc9-41ba-4574-ad71-e25c1442c8af   \n",
              "4      c754dec7-c5a3-4337-9892-c02158475064   \n",
              "...                                     ...   \n",
              "14311  f89dd9fa-07af-4384-aa0c-0d14602c0cea   \n",
              "14312  b3498176-8832-4033-aea6-b5ea85ea04c4   \n",
              "14313  f77eb51f-c3ac-420b-9586-cb187849c321   \n",
              "14314  ab59bcdd-7b7c-4107-93f5-0ccaf749236c   \n",
              "14315  fd23e7e0-a5d2-4f98-992d-9209c85153bb   \n",
              "\n",
              "                                               pub_title  \\\n",
              "0      The Impact of Dual Enrollment on College Degre...   \n",
              "1      Educational Attainment of High School Dropouts...   \n",
              "2      Differences in Outcomes for Female and Male St...   \n",
              "3      Stepping Stone and Option Value in a Model of ...   \n",
              "4      Parental Effort, School Resources, and Student...   \n",
              "...                                                  ...   \n",
              "14311  Artificial Intelligence of COVID-19 Imaging: A...   \n",
              "14312  RSNA International Trends: A Global Perspectiv...   \n",
              "14313  MCCS: a novel recognition pattern-based method...   \n",
              "14314  Quantitative Structure–Activity Relationship M...   \n",
              "14315  A ligand-based computational drug repurposing ...   \n",
              "\n",
              "                                           dataset_title  \\\n",
              "0                  National Education Longitudinal Study   \n",
              "1                  National Education Longitudinal Study   \n",
              "2                  National Education Longitudinal Study   \n",
              "3                  National Education Longitudinal Study   \n",
              "4                  National Education Longitudinal Study   \n",
              "...                                                  ...   \n",
              "14311  RSNA International COVID-19 Open Radiology Dat...   \n",
              "14312  RSNA International COVID-19 Open Radiology Dat...   \n",
              "14313  CAS COVID-19 antiviral candidate compounds dat...   \n",
              "14314  CAS COVID-19 antiviral candidate compounds dat...   \n",
              "14315  CAS COVID-19 antiviral candidate compounds dat...   \n",
              "\n",
              "                                           dataset_label  \\\n",
              "0                  National Education Longitudinal Study   \n",
              "1                  National Education Longitudinal Study   \n",
              "2                  National Education Longitudinal Study   \n",
              "3                  National Education Longitudinal Study   \n",
              "4                  National Education Longitudinal Study   \n",
              "...                                                  ...   \n",
              "14311  RSNA International COVID-19 Open Radiology Dat...   \n",
              "14312   RSNA International COVID Open Radiology Database   \n",
              "14313  CAS COVID-19 antiviral candidate compounds dat...   \n",
              "14314  CAS COVID-19 antiviral candidate compounds dat...   \n",
              "14315  CAS COVID-19 antiviral candidate compounds dat...   \n",
              "\n",
              "                                           cleaned_label  \n",
              "0                  national education longitudinal study  \n",
              "1                  national education longitudinal study  \n",
              "2                  national education longitudinal study  \n",
              "3                  national education longitudinal study  \n",
              "4                  national education longitudinal study  \n",
              "...                                                  ...  \n",
              "14311  rsna international covid 19 open radiology dat...  \n",
              "14312   rsna international covid open radiology database  \n",
              "14313  cas covid 19 antiviral candidate compounds dat...  \n",
              "14314  cas covid 19 antiviral candidate compounds dat...  \n",
              "14315  cas covid 19 antiviral candidate compounds dat...  \n",
              "\n",
              "[14316 rows x 5 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load Data Rubric\n",
        "train_pd = pd.read_csv('/cluster/tufts/data/train.csv')\n",
        "train_pd.drop_duplicates(subset =\"Id\", keep = 'first', inplace = True)# there are duplicate publications in the dataset\n",
        "train_pd = train_pd.reset_index(drop=True)\n",
        "train_pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZcAB7r25BPJ",
        "outputId": "00eefa22-e4e4-4a27-ef42-0de540ade5c5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>PredictionString</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2100032a-7c33-4bff-97ef-690822c43466</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2f392438-e215-4169-bebf-21ac4ff253e1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3f316b38-1a24-45a9-8d8c-4e05a42257c6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     Id  PredictionString\n",
              "0  2100032a-7c33-4bff-97ef-690822c43466               NaN\n",
              "1  2f392438-e215-4169-bebf-21ac4ff253e1               NaN\n",
              "2  3f316b38-1a24-45a9-8d8c-4e05a42257c6               NaN\n",
              "3  8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60               NaN"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "s = pd.read_csv('/cluster/tufts/data/sample_submission.csv')\n",
        "s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lNOrqZ95BPJ"
      },
      "source": [
        "#### Peek at Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAMcOQ-85BPJ"
      },
      "outputs": [],
      "source": [
        "#how to peek into a section in an article in the data\n",
        "f = open('/cluster/tufts/data/train/' + train_pd['Id'][0] + '.json',)\n",
        "\n",
        "# returns JSON object as\n",
        "# a dictionary\n",
        "data = json.load(f)\n",
        "print(data[0]) #this is the text in the first section of the first article"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po4LcI_65BPJ"
      },
      "source": [
        "#### Merge text from all sections in the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxVOVQBu5BPK",
        "outputId": "2e5886c0-d1f1-41e6-f215-8ecfc048e2f9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 14316/14316 [04:15<00:00, 56.12it/s]\n"
          ]
        }
      ],
      "source": [
        "# Merge all the texts in different sections into one full chunk for each article\n",
        "content = []\n",
        "sub_content = []\n",
        "for i in tqdm(range(0,len(train_pd))):\n",
        "    f = open('/cluster/tufts/data/train/' + train_pd['Id'][i] + '.json',)\n",
        "    data = json.load(f)\n",
        "    d = ''\n",
        "    for j in range(0,len(data)):\n",
        "        d = d + data[j]['text']+\" \"\n",
        "    content.append(d)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zG9XSM145BPK",
        "outputId": "d04407df-3386-48ba-f19d-0201e623e821"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Raw Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This study used data from the National Educati...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Dropping out of high school is not necessarily...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>, stress satisfactory outcomes for all youth,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Federal Reserve Bank of Richmond S1. Accountin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>This article investigates an important factor ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14311</th>\n",
              "      <td>T he coronavirus disease 2019 (COVID-19) pande...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14312</th>\n",
              "      <td>Our lives have been fundamentally altered this...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14313</th>\n",
              "      <td>The outbreak of the coronavirus disease 2019 ,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14314</th>\n",
              "      <td>The ongoing COVID-19 pandemic has challenged t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14315</th>\n",
              "      <td>deployment of approximative mathematical model...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14316 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Raw Text\n",
              "0      This study used data from the National Educati...\n",
              "1      Dropping out of high school is not necessarily...\n",
              "2       , stress satisfactory outcomes for all youth,...\n",
              "3      Federal Reserve Bank of Richmond S1. Accountin...\n",
              "4      This article investigates an important factor ...\n",
              "...                                                  ...\n",
              "14311  T he coronavirus disease 2019 (COVID-19) pande...\n",
              "14312  Our lives have been fundamentally altered this...\n",
              "14313  The outbreak of the coronavirus disease 2019 ,...\n",
              "14314  The ongoing COVID-19 pandemic has challenged t...\n",
              "14315  deployment of approximative mathematical model...\n",
              "\n",
              "[14316 rows x 1 columns]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Transfer articles into a dataframe so it can be pickled\n",
        "articles = pd.DataFrame(data=[content])\n",
        "articles = articles.T\n",
        "articles.columns = [\"Raw Text\"]\n",
        "articles.to_pickle('/cluster/tufts/data/Raw_text_data_2.pkl')\n",
        "articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWiS6ys05BPK"
      },
      "outputs": [],
      "source": [
        "articles = pd.read_pickle('/cluster/tufts/data/Raw_text_data.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drEuWzK65BPK"
      },
      "source": [
        "#### Break down articles in to sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kX_sOxDh5BPK"
      },
      "outputs": [],
      "source": [
        "#break articles down into sentence tokens\n",
        "sent_tokens = []\n",
        "\n",
        "for i in tqdm(range(0,len(articles))):\n",
        "    sent_tokens.append(sent_tokenize(articles['Raw Text'][i]))\n",
        "\n",
        "sent_token_articles = pd.DataFrame(data=[sent_tokens])\n",
        "sent_token_articles = sent_token_articles.T\n",
        "sent_token_articles.columns = [\"Sentence Tokenized Text\"]\n",
        "sent_token_articles.to_pickle('/cluster/tufts/data/sent_tokenized_text_data.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVZuXjXG5BPK",
        "outputId": "0c7aec45-1a64-49a3-b0ca-e6e45047c944"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence Tokenized Text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[This study used data from the National Educat...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[Dropping out of high school is not necessaril...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[ , stress satisfactory outcomes for all youth...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[Federal Reserve Bank of Richmond S1., Account...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[This article investigates an important factor...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14311</th>\n",
              "      <td>[T he coronavirus disease 2019 (COVID-19) pand...</td>\n",
              "      <td>RSNA International COVID-19 Open Radiology Dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14312</th>\n",
              "      <td>[Our lives have been fundamentally altered thi...</td>\n",
              "      <td>RSNA International COVID Open Radiology Database</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14313</th>\n",
              "      <td>[The outbreak of the coronavirus disease 2019 ...</td>\n",
              "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14314</th>\n",
              "      <td>[The ongoing COVID-19 pandemic has challenged ...</td>\n",
              "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14315</th>\n",
              "      <td>[deployment of approximative mathematical mode...</td>\n",
              "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14316 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 Sentence Tokenized Text  \\\n",
              "0      [This study used data from the National Educat...   \n",
              "1      [Dropping out of high school is not necessaril...   \n",
              "2      [ , stress satisfactory outcomes for all youth...   \n",
              "3      [Federal Reserve Bank of Richmond S1., Account...   \n",
              "4      [This article investigates an important factor...   \n",
              "...                                                  ...   \n",
              "14311  [T he coronavirus disease 2019 (COVID-19) pand...   \n",
              "14312  [Our lives have been fundamentally altered thi...   \n",
              "14313  [The outbreak of the coronavirus disease 2019 ...   \n",
              "14314  [The ongoing COVID-19 pandemic has challenged ...   \n",
              "14315  [deployment of approximative mathematical mode...   \n",
              "\n",
              "                                                   label  \n",
              "0                  National Education Longitudinal Study  \n",
              "1                  National Education Longitudinal Study  \n",
              "2                  National Education Longitudinal Study  \n",
              "3                  National Education Longitudinal Study  \n",
              "4                  National Education Longitudinal Study  \n",
              "...                                                  ...  \n",
              "14311  RSNA International COVID-19 Open Radiology Dat...  \n",
              "14312   RSNA International COVID Open Radiology Database  \n",
              "14313  CAS COVID-19 antiviral candidate compounds dat...  \n",
              "14314  CAS COVID-19 antiviral candidate compounds dat...  \n",
              "14315  CAS COVID-19 antiviral candidate compounds dat...  \n",
              "\n",
              "[14316 rows x 2 columns]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#sent_token_articles = pd.read_pickle('/cluster/tufts/data/sent_tokenized_text_data.pkl')\n",
        "sent_token_articles['label'] = train_pd['dataset_label'].to_list()\n",
        "sent_token_articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsLQ4W3_5BPL"
      },
      "outputs": [],
      "source": [
        "clean Sentence Tokenized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC-JvYFK5BPL"
      },
      "source": [
        "#### Get the sentences that have the referenced data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zP-nxAX5BPL",
        "outputId": "05538975-0739-42a1-b9af-f73fe7580362"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 14316/14316 [00:37<00:00, 379.78it/s]\n"
          ]
        }
      ],
      "source": [
        "# Note: more than one sentence may have the referenced article in each paper (i.e the data may be referenced more than once)\n",
        "main_list = []\n",
        "sub_list = []\n",
        "for i in tqdm(range(0,len(sent_token_articles))):\n",
        "    for j in range(0,len(sent_token_articles['Sentence Tokenized Text'][i])):\n",
        "        if (sent_token_articles['Sentence Tokenized Text'][i][j].find(train_pd['cleaned_label'][i]) != -1):\n",
        "            sub_list.append(sent_token_articles['Sentence Tokenized Text'][i][j])\n",
        "    main_list.append(sub_list)\n",
        "    sub_list = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Z6e5rH05BPL"
      },
      "source": [
        "### Second method of extracting verbs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtTUPTC15BPL",
        "outputId": "b35b9051-84ef-4494-8d7d-97c966d0af3a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 14316/14316 [00:23<00:00, 606.42it/s]\n"
          ]
        }
      ],
      "source": [
        "# Note: more than one sentence may have the referenced article in each paper (i.e the data may be referenced more than once)\n",
        "main_list = []\n",
        "sub_list = []\n",
        "isword_in_label = True\n",
        "for i in tqdm(range(0,len(sent_token_articles))):\n",
        "    label_text = train_pd['cleaned_label'][i].split(' ')\n",
        "    for j in range(0,len(sent_token_articles['Sentence Tokenized Text'][i])):\n",
        "        for k in range(0,len(label_text)):\n",
        "            isword_in_label = (isword_in_label and sent_token_articles['Sentence Tokenized Text'][i][j].find(label_text[k]) != -1)\n",
        "        if isword_in_label == True:\n",
        "            sub_list.append(sent_token_articles['Sentence Tokenized Text'][i][j])\n",
        "        isword_in_label = True\n",
        "    main_list.append(sub_list)\n",
        "    sub_list = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DToPXizj5BPL"
      },
      "outputs": [],
      "source": [
        "temp = []\n",
        "isword_in_label = True\n",
        "label_text = train_pd['cleaned_label'][1].split(' ')\n",
        "for j in range(0,len(sent_token_articles['Sentence Tokenized Text'][1])):\n",
        "    for k in range(0,len(label_text)):\n",
        "        isword_in_label = (isword_in_label and sent_token_articles['Sentence Tokenized Text'][1][j].find(label_text[k]) != -1)\n",
        "    if isword_in_label == True:\n",
        "        temp.append(sent_token_articles['Sentence Tokenized Text'][1][j])\n",
        "    isword_in_label = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZp0_HlI5BPL",
        "outputId": "a6faa907-f626-4694-9a26-2060a102f6ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['using data on public and private school students from the national education longitudinal study of 1988 (nels:88), berktold, geis, and kaufman (1998) examined the educational attainment of the 21 percent of 1988 eighth graders who had dropped out of high school at least once between eighth grade and the spring of 1994, 2 years after they would have graduated if they had finished with the majority of their cohort.',\n",
              " 'source: u.s. department of education, national center for education statistics, national education longitudinal study of 1988 (nels:88/2000).']"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTxlGK2H5BPL",
        "outputId": "43392d36-40e1-429c-9657-15493a7deaa6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "isword_in_label = True\n",
        "isword_in_label and sent_token_articles['Sentence Tokenized Text'][1][1].find(label_text[2]) != -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAy8RLpJ5BPM",
        "outputId": "786bceb9-156a-4633-8857-87238190703d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'source: u.s. department of education, national center for education statistics, national education longitudinal study of 1988 (nels:88/2000).'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sent_token_articles['Sentence Tokenized Text'][1][15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkLWLZS45BPM"
      },
      "outputs": [],
      "source": [
        "# sents_w_labels = pd.DataFrame(data=[main_list])\n",
        "# sents_w_labels = sents_w_labels.T\n",
        "# sents_w_labels.columns = [\"Sentences With Labels\"]\n",
        "# sents_w_labels\n",
        "sents_w_labels.to_pickle('/cluster/tufts/data/sents_with_labels.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LU2Ar7Bd5BPM",
        "outputId": "89a258c8-47c0-42f3-c500-20d1fe4a8f0c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['This study used data from the National Education Longitudinal Study (NELS:88) to examine the effects of dual enrollment programs for high school students on college degree attainment.',\n",
              " 'Any college degree attainment The study author collected information on college degree attainment from the fourth follow-up of the National Education Longitudinal Study collected in 2000.',\n",
              " \"The study author collected information on bachelor's degree attainment from the fourth follow-up of the National Education Longitudinal Study collected in 2000.\"]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sents_w_labels  = pd.read_pickle('/cluster/tufts/data/sents_with_labels.pkl')\n",
        "sents_w_labels[\"Sentences With Labels\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6FR5hv_5BPM",
        "outputId": "6fc1dce6-28d6-439f-e6d4-ba2afd79cf65"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 19661/19661 [00:06<00:00, 3064.03it/s]\n"
          ]
        }
      ],
      "source": [
        "main_l = []\n",
        "id_ = []\n",
        "for i in tqdm(range(0,len(sent_data))):\n",
        "    for j in range(0,len(sent_data['Tokenized Articles'][i])):\n",
        "        if (sent_data['Tokenized Articles'][i][j].find(train_pd['dataset_label'][i]) != -1):\n",
        "            main_l.append(sent_data['Tokenized Articles'][i][j])\n",
        "            id_.append(i)\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vhy960E_5BPM",
        "outputId": "09ba770e-71b0-409c-93d0-d87fa805a18f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4120"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(main_l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrMEMG555BPM"
      },
      "outputs": [],
      "source": [
        "#get list of unique labels in the dataset\n",
        "dd = train_pd['dataset_label'].to_list()\n",
        "dd = list(np.unique(dd))\n",
        "dd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aN1HCW2v5BPM"
      },
      "source": [
        "#### create bow for verbs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lv6mWMis5BPM"
      },
      "outputs": [],
      "source": [
        "#Load sentences with labels\n",
        "sents_w_labels = pd.read_pickle('/cluster/tufts/data/sents_with_labels.pkl')\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0BLTfU15BPN",
        "outputId": "d7df0e1d-555c-4b91-c575-5037cb7943ce"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 19661/19661 [08:19<00:00, 39.40it/s] \n"
          ]
        }
      ],
      "source": [
        "# Go through all the sentences with labels and tag POS\n",
        "main_list = []\n",
        "sub_list = []\n",
        "for i in tqdm(range(0,len(sents_w_labels))):\n",
        "    for j in range(0,len(sents_w_labels['Sentences With Labels'][i])):\n",
        "        dd = nlp(sents_w_labels['Sentences With Labels'][i][j])\n",
        "        sub_list.append(dd)\n",
        "\n",
        "    main_list.append(sub_list)\n",
        "    sub_list = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63-aejCd5BPN"
      },
      "outputs": [],
      "source": [
        "# save the parts of speech labelled sentences\n",
        "sents_w_labels_spacy = pd.DataFrame(data=[main_list])\n",
        "sents_w_labels_spacy = sents_w_labels_spacy.T\n",
        "sents_w_labels_spacy.columns = [\"POS Tagged Sentences With Labels\"]\n",
        "sents_w_labels_spacy.to_pickle('/cluster/tufts/data/sents_with_labels_spacy.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLbc2g_a5BPN",
        "outputId": "443118cb-577b-4ffa-9e03-57756a9f3a39"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 19661/19661 [00:00<00:00, 19976.59it/s]\n"
          ]
        }
      ],
      "source": [
        "# Go through all the sentences with labels and extract the verbs\n",
        "main_list = []\n",
        "sub_list = []\n",
        "for i in tqdm(range(0,len(sents_w_labels_spacy))):\n",
        "    for j in range(0,len(sents_w_labels_spacy[\"POS Tagged Sentences With Labels\"][i])):\n",
        "        dd = [let_me_tize_u.lemmatize(token.text,pos='v').lower() for token in sents_w_labels_spacy[\"POS Tagged Sentences With Labels\"][i][j] if token.pos_ == 'VERB']\n",
        "        sub_list.append(dd)\n",
        "\n",
        "    main_list.append(sub_list)\n",
        "    sub_list = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUu5uzgv5BPN"
      },
      "outputs": [],
      "source": [
        "# Go through all the verbs and re-lemmatize (I didn't get good lemmatization in the previous loop)\n",
        "main_list = []\n",
        "sub_list = []\n",
        "for i in tqdm(range(0,len(sents_w_labels_spacy_verbs))):\n",
        "    for j in range(0,len(sents_w_labels_spacy_verbs[\"Verbs in Sentences With Labels\"][i])):\n",
        "        dd = [let_me_tize_u.lemmatize(token,pos='v') for token in sents_w_labels_spacy_verbs[\"Verbs in Sentences With Labels\"][i][j]]\n",
        "        sub_list.append(dd)\n",
        "\n",
        "    main_list.append(sub_list)\n",
        "    sub_list = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhmUNEzn5BPN"
      },
      "outputs": [],
      "source": [
        "# save the parts of speech labelled sentences\n",
        "sents_w_labels_spacy_verbs = pd.DataFrame(data=[main_list])\n",
        "sents_w_labels_spacy_verbs = sents_w_labels_spacy_verbs.T\n",
        "sents_w_labels_spacy_verbs.columns = [\"Verbs in Sentences With Labels\"]\n",
        "sents_w_labels_spacy_verbs.to_pickle('/cluster/tufts/data/sents_with_labels_spacy_verbs.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tDsxsyA5BPN"
      },
      "outputs": [],
      "source": [
        "#sents_w_labels_spacy_verbs = pd.read_pickle('/cluster/tufts/data/sents_with_labels_spacy_verbs.pkl')\n",
        "sent_token_articles = pd.read_pickle('/cluster/tufts/data/sent_tokenized_text_data.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsf1USMF5BPP"
      },
      "outputs": [],
      "source": [
        "# create list of unique verbs for each sentence with a label\n",
        "main_list = []\n",
        "\n",
        "for i in range(0,len(sents_w_labels_spacy_verbs)):\n",
        "    new_list = []\n",
        "    for j in range(0,len(sents_w_labels_spacy_verbs[\"Verbs in Sentences With Labels\"][i])):\n",
        "        new_list = new_list + sents_w_labels_spacy_verbs[\"Verbs in Sentences With Labels\"][i][j]\n",
        "    final_list = list(np.unique(new_list))\n",
        "    main_list.append(final_list)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsTwpOwS5BPP"
      },
      "source": [
        "### An Example of the quality of verbs discovered by Spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLaUoqo65BPP",
        "outputId": "ca20ec3b-2b06-4bbf-addf-e74ce339ed30"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Verbs</th>\n",
              "      <th>Occurrences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>#</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>'d</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>):</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-</td>\n",
              "      <td>86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-(c</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-------*----g------------g--------------------...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-.-----v-----d--------d</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-136</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-17</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>-50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>-66</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>-although</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>-amyloid</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>-an</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>-at</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>-based</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>-brainprint</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>-can</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>-confirm</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>-continued</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>-data</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>-demonstrating</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>-despite</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>-developed</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>-differentiated</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>-each</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>-exemplify</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>-generalize</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>-had</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>-have</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>-including</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>-incofish</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>-late</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>-lowering</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>-matched</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>-missing</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>-npsas:93</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>-oecd</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>-permit</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>-producing</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>-regularized</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>-rented</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>-report</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>-reported</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>-resulting</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>-revised</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>-running</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>-state</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>-that</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>-thereby</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Verbs  Occurrences\n",
              "0                                                   #            1\n",
              "1                                                  'd            1\n",
              "2                                                  ):            1\n",
              "3                                                   -           86\n",
              "4                                                 -(c            1\n",
              "5   -------*----g------------g--------------------...            1\n",
              "6                             -.-----v-----d--------d            1\n",
              "7                                                -136            1\n",
              "8                                                 -17            1\n",
              "9                                                 -50            1\n",
              "10                                                -66            1\n",
              "11                                          -although            1\n",
              "12                                           -amyloid            1\n",
              "13                                                -an            2\n",
              "14                                                -at            1\n",
              "15                                             -based            3\n",
              "16                                        -brainprint            2\n",
              "17                                               -can            1\n",
              "18                                           -confirm            1\n",
              "19                                         -continued            1\n",
              "20                                              -data            1\n",
              "21                                     -demonstrating            1\n",
              "22                                           -despite            2\n",
              "23                                         -developed            1\n",
              "24                                    -differentiated            1\n",
              "25                                              -each            1\n",
              "26                                         -exemplify            1\n",
              "27                                        -generalize            1\n",
              "28                                               -had            1\n",
              "29                                              -have            1\n",
              "30                                         -including            1\n",
              "31                                          -incofish            2\n",
              "32                                              -late            1\n",
              "33                                          -lowering            1\n",
              "34                                           -matched            1\n",
              "35                                           -missing            1\n",
              "36                                          -npsas:93            2\n",
              "37                                              -oecd            1\n",
              "38                                            -permit            1\n",
              "39                                         -producing            1\n",
              "40                                       -regularized            1\n",
              "41                                            -rented            1\n",
              "42                                            -report            2\n",
              "43                                          -reported            1\n",
              "44                                         -resulting            2\n",
              "45                                           -revised            2\n",
              "46                                           -running            1\n",
              "47                                             -state            1\n",
              "48                                              -that            1\n",
              "49                                           -thereby            2"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sents_w_labels_spacy_verbs = pd.read_pickle('/cluster/tufts/data/sents_with_labels_spacy_verbs.pkl')\n",
        "\n",
        "main_list = []\n",
        "\n",
        "for i in range(0,len(sents_w_labels_spacy_verbs)):\n",
        "    new_list = []\n",
        "    for j in range(0,len(sents_w_labels_spacy_verbs[\"Verbs in Sentences With Labels\"][i])):\n",
        "        new_list = new_list + sents_w_labels_spacy_verbs[\"Verbs in Sentences With Labels\"][i][j]\n",
        "    final_list = list(np.unique(new_list))\n",
        "    main_list.append(final_list)\n",
        "\n",
        "\n",
        "sents_v = pd.DataFrame(data=[main_list])\n",
        "sents_v= sents_v.T\n",
        "sents_v.columns = [\"Verbs\"]\n",
        "\n",
        "convert_v_list = sents_v['Verbs'].to_list()\n",
        "\n",
        "v_list = []\n",
        "for i in range(0,len(convert_v_list)):\n",
        "    v_list = v_list + convert_v_list[i]\n",
        "unique_v_list = list(np.unique(v_list))\n",
        "\n",
        "occurrences = []\n",
        "for i in range(0,len(unique_v_list)):\n",
        "    occurrences.append(v_list.count(unique_v_list[i]))\n",
        "\n",
        "bow_df = pd.DataFrame(columns=['Verbs','Occurrences'])\n",
        "bow_df['Verbs'] = unique_v_list\n",
        "bow_df['Occurrences'] = occurrences\n",
        "bow_df.head(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fITJHbxI5BPP",
        "outputId": "cdd1fa25-5180-4b55-ca05-ab4c53523d8f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 14316/14316 [00:50<00:00, 284.11it/s]\n"
          ]
        }
      ],
      "source": [
        "# create data frame with sentences from all articles in rows\n",
        "main_list = []\n",
        "labels = []\n",
        "\n",
        "for i in tqdm(range(0,len(sent_token_articles))):\n",
        "    for j in range(0,len(sent_token_articles['Sentence Tokenized Text'][i])):\n",
        "        main_list.append(sent_token_articles['Sentence Tokenized Text'][i][j])\n",
        "        if (sent_token_articles['Sentence Tokenized Text'][i][j].find(sent_token_articles['label'][i]) != -1):\n",
        "            labels.append(1)\n",
        "        else:\n",
        "            labels.append(0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29RdS7sX5BPP",
        "outputId": "8edbd347-d58f-4f06-ae51-b2bc78dfc717"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4178921"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjOnzQuR5BPP",
        "outputId": "ad0b5f08-c95a-45d9-bd81-1234faf18705"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentences</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This study used data from the National Educati...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The study also reported whether the impacts of...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>In addition, a supplemental analysis reports o...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Dual enrollment programs offer college-level l...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The programs offer college courses and/or the ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4178916</th>\n",
              "      <td>As a consequence of the current COVID-19 pande...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4178917</th>\n",
              "      <td>As a side effect of analyzing the data, we are...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4178918</th>\n",
              "      <td>In addition, the workflow has been used to det...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4178919</th>\n",
              "      <td>The material has been used successfully for te...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4178920</th>\n",
              "      <td>The workflows, tutorials, and the information ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4178921 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 Sentences  labels\n",
              "0        This study used data from the National Educati...       1\n",
              "1        The study also reported whether the impacts of...       0\n",
              "2        In addition, a supplemental analysis reports o...       0\n",
              "3        Dual enrollment programs offer college-level l...       0\n",
              "4        The programs offer college courses and/or the ...       0\n",
              "...                                                    ...     ...\n",
              "4178916  As a consequence of the current COVID-19 pande...       0\n",
              "4178917  As a side effect of analyzing the data, we are...       0\n",
              "4178918  In addition, the workflow has been used to det...       0\n",
              "4178919  The material has been used successfully for te...       0\n",
              "4178920  The workflows, tutorials, and the information ...       0\n",
              "\n",
              "[4178921 rows x 2 columns]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_sents = pd.DataFrame(data=[main_list])\n",
        "all_sents = all_sents.T\n",
        "all_sents.columns = [\"Sentences\"]\n",
        "all_sents['labels'] = labels\n",
        "all_sents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5KEgJdA5BPQ"
      },
      "outputs": [],
      "source": [
        "all_sents.to_pickle('/cluster/tufts/data/all_sents.pkl')\n",
        "#sents_v.to_pickle('/cluster/tufts/data/sents_v.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0y5V1rJ25BPQ"
      },
      "outputs": [],
      "source": [
        "df.to_pickle('/cluster/tufts/data/sents_with_data.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VutAIpw95BPQ"
      },
      "source": [
        "### New method of getting the verbs with wordnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRK6nVPd5BPQ",
        "outputId": "7c6304f9-aaf5-4d5a-a48d-8accba06dd77"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentences With Labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[this study used data from the national educat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[using data on public and private school stude...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[using the nationally representative, longitud...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[national education longitudinal study of 1988...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[using data from the national education longit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14311</th>\n",
              "      <td>[the rsna has developed the medical imaging an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14312</th>\n",
              "      <td>[the international community came together to ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14313</th>\n",
              "      <td>[then, several 3cl pro  ligand complexes are u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14314</th>\n",
              "      <td>[we used the resulting models to screen 1087 f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14315</th>\n",
              "      <td>[after data cleaning and chemical structure st...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14316 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   Sentences With Labels\n",
              "0      [this study used data from the national educat...\n",
              "1      [using data on public and private school stude...\n",
              "2      [using the nationally representative, longitud...\n",
              "3      [national education longitudinal study of 1988...\n",
              "4      [using data from the national education longit...\n",
              "...                                                  ...\n",
              "14311  [the rsna has developed the medical imaging an...\n",
              "14312  [the international community came together to ...\n",
              "14313  [then, several 3cl pro  ligand complexes are u...\n",
              "14314  [we used the resulting models to screen 1087 f...\n",
              "14315  [after data cleaning and chemical structure st...\n",
              "\n",
              "[14316 rows x 1 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#load sentences with labels\n",
        "\n",
        "sents_w_labels  = pd.read_pickle('/cluster/tufts/data/sents_with_labels.pkl')\n",
        "sents_w_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFfwB2B85BPQ"
      },
      "outputs": [],
      "source": [
        "# Check if all sentences with labels were gotten\n",
        "empty = []\n",
        "for i in range(0,len(sents_w_labels)):\n",
        "    if len(sents_w_labels['Sentences With Labels'][i]) == 0:\n",
        "        empty.append(i)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcmE4HH95BPQ",
        "outputId": "412142aa-b9e3-47c4-908c-a38bcceb114d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 13188th data label is different from cleaned_label: shorter\n",
        "# 13755th data not referenced in the same manner as label (acually very different)\n",
        "# 13924th data not referenced in the same manner as label (acually very different)\n",
        "empty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uxx4SYTD5BPQ"
      },
      "outputs": [],
      "source": [
        "target = 'covid 19 open research dataset'\n",
        "for i in range(0,len(sent_token_articles['Sentence Tokenized Text'][13188])):\n",
        "    if sent_token_articles['Sentence Tokenized Text'][13188][i].find(target) != -1:\n",
        "        sentence = sent_token_articles['Sentence Tokenized Text'][13188][i]\n",
        "        #break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bvp0gj-u5BPQ",
        "outputId": "55a5b969-0779-43fd-85d8-20e26f59d28b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence Tokenized Text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[This study used data from the National Educat...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[Dropping out of high school is not necessaril...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[ , stress satisfactory outcomes for all youth...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[Federal Reserve Bank of Richmond S1., Account...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[This article investigates an important factor...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14311</th>\n",
              "      <td>[T he coronavirus disease 2019 (COVID-19) pand...</td>\n",
              "      <td>RSNA International COVID-19 Open Radiology Dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14312</th>\n",
              "      <td>[Our lives have been fundamentally altered thi...</td>\n",
              "      <td>RSNA International COVID Open Radiology Database</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14313</th>\n",
              "      <td>[The outbreak of the coronavirus disease 2019 ...</td>\n",
              "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14314</th>\n",
              "      <td>[The ongoing COVID-19 pandemic has challenged ...</td>\n",
              "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14315</th>\n",
              "      <td>[deployment of approximative mathematical mode...</td>\n",
              "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14316 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 Sentence Tokenized Text  \\\n",
              "0      [This study used data from the National Educat...   \n",
              "1      [Dropping out of high school is not necessaril...   \n",
              "2      [ , stress satisfactory outcomes for all youth...   \n",
              "3      [Federal Reserve Bank of Richmond S1., Account...   \n",
              "4      [This article investigates an important factor...   \n",
              "...                                                  ...   \n",
              "14311  [T he coronavirus disease 2019 (COVID-19) pand...   \n",
              "14312  [Our lives have been fundamentally altered thi...   \n",
              "14313  [The outbreak of the coronavirus disease 2019 ...   \n",
              "14314  [The ongoing COVID-19 pandemic has challenged ...   \n",
              "14315  [deployment of approximative mathematical mode...   \n",
              "\n",
              "                                                   label  \n",
              "0                  National Education Longitudinal Study  \n",
              "1                  National Education Longitudinal Study  \n",
              "2                  National Education Longitudinal Study  \n",
              "3                  National Education Longitudinal Study  \n",
              "4                  National Education Longitudinal Study  \n",
              "...                                                  ...  \n",
              "14311  RSNA International COVID-19 Open Radiology Dat...  \n",
              "14312   RSNA International COVID Open Radiology Database  \n",
              "14313  CAS COVID-19 antiviral candidate compounds dat...  \n",
              "14314  CAS COVID-19 antiviral candidate compounds dat...  \n",
              "14315  CAS COVID-19 antiviral candidate compounds dat...  \n",
              "\n",
              "[14316 rows x 2 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sent_token_articles = pd.read_pickle('/cluster/tufts/data/sent_tokenized_text_data.pkl')\n",
        "sent_token_articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyRs-ERj5BPR",
        "outputId": "bcb77b7d-9636-476c-8df2-14c64251b31b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 14316/14316 [00:00<00:00, 42007.80it/s]\n"
          ]
        }
      ],
      "source": [
        "# a little preprocessing\n",
        "sub_list = []\n",
        "for i in tqdm(range(0,len(sent_token_articles))):\n",
        "    for j in range(0,len(sent_token_articles['Sentence Tokenized Text'][i])):\n",
        "        content = sent_token_articles['Sentence Tokenized Text'][i][j].lower()\n",
        "        content = content.replace('-', ' ')\n",
        "        content = content.replace('(', ' ')\n",
        "        content = content.replace(')', ' ')\n",
        "        sub_list.append(content)\n",
        "\n",
        "\n",
        "    sent_token_articles['Sentence Tokenized Text'][i] = sub_list\n",
        "    sub_list = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2G4Hr2C5BPR",
        "outputId": "131f349b-352f-46d8-f1b5-3b5879896271"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4178921"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXB-Bwc35BPR"
      },
      "outputs": [],
      "source": [
        "word_token_sents = pd.DataFrame(data=[main_list])\n",
        "word_token_sents = word_token_sents.T\n",
        "word_token_sents.columns = [\"Word Tokenized Sentences With Labels\"]\n",
        "#word_token_sents['labels'] = labels\n",
        "word_token_sents\n",
        "word_token_sents.to_pickle('/cluster/tufts/data/word_token_sents_with_labels.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDhVV_M15BPR",
        "outputId": "839c67aa-aa0f-4ba6-d675-13657c350877"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word Tokenized Sentences With Labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[[this, study, used, data, from, the, national...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[[using, data, public, and, private, school, s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[[using, the, nationally, representative, long...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[[national, education, longitudinal, study, 19...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[[using, data, from, the, national, education,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14311</th>\n",
              "      <td>[[the, rsna, has, developed, the, medical, ima...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14312</th>\n",
              "      <td>[[the, international, community, came, togethe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14313</th>\n",
              "      <td>[[then, several, 3cl, pro, ligand, complexes, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14314</th>\n",
              "      <td>[[used, the, resulting, models, screen, 1087, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14315</th>\n",
              "      <td>[[after, data, cleaning, and, chemical, struct...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14316 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                    Word Tokenized Sentences With Labels\n",
              "0      [[this, study, used, data, from, the, national...\n",
              "1      [[using, data, public, and, private, school, s...\n",
              "2      [[using, the, nationally, representative, long...\n",
              "3      [[national, education, longitudinal, study, 19...\n",
              "4      [[using, data, from, the, national, education,...\n",
              "...                                                  ...\n",
              "14311  [[the, rsna, has, developed, the, medical, ima...\n",
              "14312  [[the, international, community, came, togethe...\n",
              "14313  [[then, several, 3cl, pro, ligand, complexes, ...\n",
              "14314  [[used, the, resulting, models, screen, 1087, ...\n",
              "14315  [[after, data, cleaning, and, chemical, struct...\n",
              "\n",
              "[14316 rows x 1 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_token_sents = pd.read_pickle('/cluster/tufts/data/word_token_sents_with_labels.pkl')\n",
        "word_token_sents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRL3hgju5BPR"
      },
      "source": [
        "### use newest method to extract verbs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ivChV7M5BPS",
        "outputId": "012630dd-9148-4326-9cd4-bcf72d912781"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 14316/14316 [00:19<00:00, 730.63it/s] \n"
          ]
        }
      ],
      "source": [
        "sent_token_articles = pd.read_pickle('/cluster/tufts/data/sent_tokenized_text_data_RTdatasetRT_combined.pkl')\n",
        "\n",
        "# Note: more than one sentence may have the referenced article in each paper (i.e the data may be referenced more than once)\n",
        "replacement_term = 'RTdatasetRT'\n",
        "main_list = []\n",
        "sub_list = []\n",
        "for i in tqdm(range(0,len(sent_token_articles))):\n",
        "    for j in range(0,len(sent_token_articles['Sentence Tokenized Text'][i])):\n",
        "        if (sent_token_articles['Sentence Tokenized Text'][i][j].find(replacement_term) != -1):\n",
        "            sub_list.append(sent_token_articles['Sentence Tokenized Text'][i][j])\n",
        "    main_list.append(sub_list)\n",
        "    sub_list = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3wijx1v5BPS"
      },
      "outputs": [],
      "source": [
        "sents_w_labels = pd.DataFrame(data=[main_list])\n",
        "sents_w_labels = sents_w_labels.T\n",
        "sents_w_labels.columns = [\"Sentences With Labels\"]\n",
        "sents_w_labels\n",
        "sents_w_labels.to_pickle('/cluster/tufts/data/sents_with_labels_bert.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYZVG0ur5BPS",
        "outputId": "0585bdce-6149-4538-c85a-5a68d6272205"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 14316/14316 [00:11<00:00, 1204.40it/s]\n"
          ]
        }
      ],
      "source": [
        "sents_w_labels = pd.read_pickle('/cluster/tufts/data/sents_with_labels_bert.pkl')\n",
        "\n",
        "main_list = []\n",
        "sub_list = []\n",
        "\n",
        "for i in tqdm(range(0,len(sents_w_labels))):\n",
        "    for j in range(0,len(sents_w_labels[\"Sentences With Labels\"][i])):\n",
        "        sub_list.append(word_tokenize(sents_w_labels[\"Sentences With Labels\"][i][j]))\n",
        "\n",
        "    main_list.append(sub_list)\n",
        "    sub_list = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxOLrx1_5BPS"
      },
      "outputs": [],
      "source": [
        "word_token_sents = pd.DataFrame(data=[main_list])\n",
        "word_token_sents = word_token_sents.T\n",
        "word_token_sents.columns = [\"Word Tokenized Sentences With Labels\"]\n",
        "#word_token_sents['labels'] = labels\n",
        "#word_token_sents\n",
        "word_token_sents.to_pickle('/cluster/tufts/data/word_token_sents_with_labels_bert.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWdApIhn5BPS",
        "outputId": "412a508a-05fc-40cf-a1c8-e614ee93c083"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 14316/14316 [00:57<00:00, 249.57it/s]\n"
          ]
        }
      ],
      "source": [
        "# Extract verbs using nltk's wordnet\n",
        "verb_list = []\n",
        "sub_list = []\n",
        "trivial_verbs = ['be','have']\n",
        "\n",
        "for i in tqdm(range(0,len(word_token_sents))):\n",
        "    for j in range(0,len(word_token_sents[\"Word Tokenized Sentences With Labels\"][i])):\n",
        "        for k in range(0,len(word_token_sents[\"Word Tokenized Sentences With Labels\"][i][j])):\n",
        "            if word_token_sents[\"Word Tokenized Sentences With Labels\"][i][j][k] == 'rtdatasetrt':\n",
        "                sub_list.append(word_token_sents[\"Word Tokenized Sentences With Labels\"][i][j][k])\n",
        "            else:\n",
        "                lemma = let_me_tize_u.lemmatize(word_token_sents[\"Word Tokenized Sentences With Labels\"][i][j][k],pos='v')\n",
        "                if lemma not in trivial_verbs:\n",
        "                    verb_sets = []\n",
        "                    wrd = wn.synsets(word_token_sents[\"Word Tokenized Sentences With Labels\"][i][j][k], pos=wn.VERB)\n",
        "                    for item in wrd:\n",
        "                        verb_sets.append(item.lemmas()[0].name())\n",
        "\n",
        "                    noun_sets = []\n",
        "                    wrd = wn.synsets(word_token_sents[\"Word Tokenized Sentences With Labels\"][i][j][k], pos=wn.NOUN)\n",
        "                    for item in wrd:\n",
        "                        noun_sets.append(item.lemmas()[0].name())\n",
        "\n",
        "                    if len(verb_sets) > len(noun_sets):\n",
        "                        sub_list.append(word_token_sents[\"Word Tokenized Sentences With Labels\"][i][j][k])\n",
        "\n",
        "    verb_list.append(sub_list)\n",
        "    sub_list = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knFuQWZC5BPS",
        "outputId": "e1cc52f1-3098-4bf3-a73f-c1009fcf9d65"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Synset('writer.n.01'), Synset('generator.n.03')]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wn.synsets('author',pos=wn.NOUN)\n",
        "#wrd[0].lemmas()[0].name()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBmqgA-S5BPS"
      },
      "outputs": [],
      "source": [
        "sents_v_wordnet = pd.DataFrame(data=[verb_list])\n",
        "sents_v_wordnet = sents_v_wordnet.T\n",
        "sents_v_wordnet.columns = [\"Verbs\"]\n",
        "sents_v_wordnet\n",
        "sents_v_wordnet.to_pickle('/cluster/tufts/data/verbs_sentences_with_labels.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEBGcJxM5BPT",
        "outputId": "87500cdd-2840-4928-cc78-4403fe2d5b0a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Verbs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[used, examine, collected, follow, up, collect...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[using, examined, dropped, out, graduated, fin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[using, set, used, examine, associated, partic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[follows, presents, requires, disentangle, fol...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[using, added, includes, include, investigate,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14311</th>\n",
              "      <td>[developed, attained, buy, contribute]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14312</th>\n",
              "      <td>[came, build, further, facilitate]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14313</th>\n",
              "      <td>[used, create, characterize, screening, carrie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14314</th>\n",
              "      <td>[used, resulting, approved, drugs, list, assig...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14315</th>\n",
              "      <td>[cleaning, extracted, set, grouped, drug, requ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14316 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   Verbs\n",
              "0      [used, examine, collected, follow, up, collect...\n",
              "1      [using, examined, dropped, out, graduated, fin...\n",
              "2      [using, set, used, examine, associated, partic...\n",
              "3      [follows, presents, requires, disentangle, fol...\n",
              "4      [using, added, includes, include, investigate,...\n",
              "...                                                  ...\n",
              "14311             [developed, attained, buy, contribute]\n",
              "14312                 [came, build, further, facilitate]\n",
              "14313  [used, create, characterize, screening, carrie...\n",
              "14314  [used, resulting, approved, drugs, list, assig...\n",
              "14315  [cleaning, extracted, set, grouped, drug, requ...\n",
              "\n",
              "[14316 rows x 1 columns]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sents_v_wordnet = pd.DataFrame(data=[verb_list])\n",
        "sents_v_wordnet = sents_v_wordnet.T\n",
        "sents_v_wordnet.columns = [\"Verbs\"]\n",
        "sents_v_wordnet.to_pickle('/cluster/tufts/data/verbs_sentences_with_labels_bert.pkl')\n",
        "sents_v_wordnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tfUHqFJ5BPT",
        "outputId": "c1919911-e033-47de-9922-77ee70511a9c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 14316/14316 [10:07<00:00, 23.57it/s]  \n"
          ]
        }
      ],
      "source": [
        "# create data frame with sentences from all articles in rows\n",
        "sent_token_articles = pd.read_pickle('/cluster/tufts/data/sent_tokenized_text_data_RTdatasetRT_combined.pkl')\n",
        "\n",
        "main_list = []\n",
        "labels = []\n",
        "\n",
        "for i in tqdm(range(0,len(sent_token_articles))):\n",
        "    for j in range(0,len(sent_token_articles['Sentence Tokenized Text'][i])):\n",
        "        #sentence = sent_token_articles['Sentence Tokenized Text'][i][j]\n",
        "        main_list.append(word_tokenize(sent_token_articles['Sentence Tokenized Text'][i][j]))\n",
        "        if  (sent_token_articles['Sentence Tokenized Text'][i][j].find('RTdatasetRT') != -1):\n",
        "            labels.append(1)\n",
        "        else:\n",
        "            labels.append(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxCZfMMW5BPT"
      },
      "outputs": [],
      "source": [
        "all_sents.to_pickle('/cluster/tufts/data/all_sents_BOW_1.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZcZWdxV5BPT"
      },
      "outputs": [],
      "source": [
        "all_sents = pd.DataFrame(data=[main_list])\n",
        "all_sents = all_sents.T\n",
        "all_sents.columns = [\"Sentences\"]\n",
        "all_sents['labels'] = labels\n",
        "#all_sents\n",
        "all_sents.to_pickle('/cluster/tufts/data/all_sents_bert.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehb68ui65BPT"
      },
      "outputs": [],
      "source": [
        "all_sents.to_pickle('/cluster/tufts/data/all_sents.pkl')\n",
        "#sents_v.to_pickle('/cluster/tufts/data/sents_v.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZ9R3oDr5BPT"
      },
      "outputs": [],
      "source": [
        "#sent_token_articles = pd.read_pickle('/cluster/tufts/data/sent_tokenized_text_data.pkl')\n",
        "#a = sent_token_articles.index\n",
        "b = list(a)[0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5dbK85-5BPT"
      },
      "outputs": [],
      "source": [
        "del dataset_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwjuWAsp5BPT",
        "outputId": "82a3b72e-3219-40ed-bf2c-93fb348d3ddf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 14316/14316 [00:00<00:00, 199871.70it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "yes\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# confirm that the number of sentences with data labels match that in the list of sentences\n",
        "count = 0\n",
        "for i in tqdm(range(0,len(sents_w_labels))):\n",
        "    for j in range(0,len(sents_w_labels['Sentences With Labels'][i])):\n",
        "        count = count + 1\n",
        "\n",
        "count2 = len(all_sents[all_sents['labels']==1])\n",
        "if count == count2:\n",
        "    print('yes')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ic233I7Q5BPT",
        "outputId": "cfe5e3b7-d76d-4c0b-80a3-11cf84d70269"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Verbs</th>\n",
              "      <th>Occurrences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>abandoned</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>abate</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>abbreviated</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>abide</td>\n",
              "      <td>89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>abort</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4970</th>\n",
              "      <td>yielded</td>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4971</th>\n",
              "      <td>yielding</td>\n",
              "      <td>43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4972</th>\n",
              "      <td>yields</td>\n",
              "      <td>58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4973</th>\n",
              "      <td>zoning</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4974</th>\n",
              "      <td>zoomed</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4975 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            Verbs  Occurrences\n",
              "0       abandoned            5\n",
              "1           abate            1\n",
              "2     abbreviated            8\n",
              "3           abide           89\n",
              "4           abort            3\n",
              "...           ...          ...\n",
              "4970      yielded           90\n",
              "4971     yielding           43\n",
              "4972       yields           58\n",
              "4973       zoning            4\n",
              "4974       zoomed            4\n",
              "\n",
              "[4975 rows x 2 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create data frame to store the unique verbs from the sentences with label and the number of occurrences\n",
        "# of these verbs in all sentences\n",
        "sents_v = pd.read_pickle('/cluster/tufts/data/verbs_sentences_with_labels_bert.pkl')\n",
        "convert_v_list = sents_v['Verbs'].to_list()\n",
        "\n",
        "v_list = []\n",
        "for i in range(0,len(convert_v_list)):\n",
        "    v_list = v_list + convert_v_list[i]\n",
        "unique_v_list = list(np.unique(v_list))\n",
        "len(unique_v_list)\n",
        "\n",
        "occurrences = []\n",
        "for i in range(0,len(unique_v_list)):\n",
        "    occurrences.append(v_list.count(unique_v_list[i]))\n",
        "\n",
        "bow_df = pd.DataFrame(columns=['Verbs','Occurrences'])\n",
        "bow_df['Verbs'] = unique_v_list\n",
        "bow_df['Occurrences'] = occurrences\n",
        "bow_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaTz0x8z5BPU",
        "outputId": "26e4e9da-3f1f-40ad-9b53-4dcb1b430077"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4178921/4178921 [1:40:04<00:00, 696.00it/s]  \n"
          ]
        }
      ],
      "source": [
        "# Create a corpus by filtering out other words that are not verbs seen in sentences with labels\n",
        "\n",
        "min_occur = 1 # set this value to filter verbs that rare show up in other sentences\n",
        "all_sents = pd.read_pickle('/cluster/tufts/data/all_sents.pkl')\n",
        "\n",
        "min_occur_vocab_df = bow_df[bow_df['Occurrences'] >= min_occur]\n",
        "min_occur_vocab = min_occur_vocab_df['Verbs'].to_list()\n",
        "min_occur_vocab\n",
        "\n",
        "# sentences filter by vocab\n",
        "sentence_list = []\n",
        "is_empty = []\n",
        "for i in tqdm(range(0,len(all_sents))):\n",
        "    tokens = [w for w in all_sents['Sentences'][i] if w in min_occur_vocab]\n",
        "    sentence_list.append(' '.join(tokens))\n",
        "    if len(tokens)==0:\n",
        "        is_empty.append(True)\n",
        "    else:\n",
        "        is_empty.append(False)\n",
        "\n",
        "all_sents['BOW'] = sentence_list\n",
        "all_sents['Empty'] = is_empty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xjxllsh5BPU"
      },
      "outputs": [],
      "source": [
        "all_sents.to_pickle('/cluster/tufts/data/all_sents_bert_BOW_1.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rWgYUZl5BPU"
      },
      "outputs": [],
      "source": [
        "all_sents = pd.read_pickle('/cluster/tufts/data/all_sents_bert_BOW_1.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aaW3guf5BPU"
      },
      "source": [
        "### SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVVQ066l5BPU"
      },
      "outputs": [],
      "source": [
        "def create_test_train(all_sents, mode, test_data_size):\n",
        "    # Remove empty sentences\n",
        "    dataset_df = all_sents[all_sents['Empty']==False]\n",
        "\n",
        "    #Get test data\n",
        "    # get and equal number of sentences with and without labels\n",
        "    test_df_1 = dataset_df[dataset_df['labels']==1].sample(n=test_data_size)\n",
        "    test_df_0 = dataset_df[dataset_df['labels']==0].sample(n=test_data_size)\n",
        "\n",
        "    # Merge two sets of sentences\n",
        "    test = [test_df_1,test_df_0]\n",
        "    test_df = pd.concat(test)\n",
        "\n",
        "    # convert test data to array\n",
        "    xtest_data = test_df['BOW'].to_list()\n",
        "    ytest = test_df['labels'].to_list()\n",
        "\n",
        "    # Remove the the data used in the test set from the data pull (separating test data from train data)\n",
        "    test_index = list(test_df.index)\n",
        "    train_df = dataset_df.drop(test_index)\n",
        "\n",
        "    xtrain_data = train_df['BOW'].to_list()\n",
        "    ytrain_data = train_df['labels'].to_list()\n",
        "\n",
        "    #convert test and train data to BOW\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(xtrain_data)\n",
        "\n",
        "    # encode training data set\n",
        "    xtrain_bow = tokenizer.texts_to_matrix(xtrain_data, mode=mode)\n",
        "\n",
        "    #tokenizer.fit_on_texts(xtest_data)\n",
        "    Xtest = tokenizer.texts_to_matrix(xtest_data, mode=mode)\n",
        "\n",
        "    #Oversampling the data to create an equal number of classes (sentences with and without data)\n",
        "    smote = SMOTE(random_state = 101)\n",
        "    Xtrain, ytrain = smote.fit_resample(xtrain_bow, ytrain_data)\n",
        "\n",
        "    return Xtrain, ytrain, Xtest, ytest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlURcuGu5BPU"
      },
      "outputs": [],
      "source": [
        "# Get the non empty sentences and group them into one with labels (1) and ones without labels (0)\n",
        "# There are 50,000 sentences with labels and about 3 million sentences without labels so I randomly select 50,000\n",
        "# without labels to balance the ratio of the two classes\n",
        "# Another (better) idea is to use SMOTE\n",
        "train_df_1 = all_sents[all_sents['Empty']==False]\n",
        "train_df_1_1 = train_df_1[train_df_1['labels']==1]\n",
        "train_df_1_2 = train_df_1[train_df_1['labels']==0]\n",
        "first_label_df = train_df_1_1['BOW']\n",
        "second_label_df = train_df_1_2['BOW'].sample(n=len(train_df_1_1)) # randomly select 50,000 sentences with label 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbD6KsfL5BPU"
      },
      "outputs": [],
      "source": [
        "# Creating the corpus takes time so I saved this step\n",
        "first_label_df = pd.read_pickle('/cluster/tufts/data/first_label_df.pkl')\n",
        "second_label_df = pd.read_pickle('/cluster/tufts/data/second_label_df.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4q6Itq9e5BPV"
      },
      "outputs": [],
      "source": [
        "# Function creates testing and training data\n",
        "def create_test_train(split, mode,first_label_df ,second_label_df):\n",
        "    test_split = split\n",
        "\n",
        "    x_first_label = first_label_df.to_list()\n",
        "    x_second_label = second_label_df.to_list()\n",
        "\n",
        "    split_index = len(x_first_label)-test_split\n",
        "\n",
        "    xtrain_data = x_second_label[0:split_index] + x_first_label[0:split_index]\n",
        "    ytrain = array([0 for _ in range(split_index)] + [1 for _ in range(split_index)])\n",
        "\n",
        "    xtest_data = x_second_label[split_index:len(x_first_label)] + x_first_label[split_index:len(x_first_label)]\n",
        "    ytest = array([0 for _ in range(len(x_first_label)-split_index)] + [1 for _ in range(len(x_first_label)-split_index)])\n",
        "\n",
        "    # fit the tokenizer on the documents\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(x_first_label+x_second_label)\n",
        "\n",
        "    # encode training data set\n",
        "    Xtrain = tokenizer.texts_to_matrix(xtrain_data, mode=mode)\n",
        "\n",
        "    #tokenizer.fit_on_texts(xtest_data)\n",
        "    Xtest = tokenizer.texts_to_matrix(xtest_data, mode=mode)\n",
        "\n",
        "    return Xtrain, ytrain, Xtest, ytest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PujcJlF5BPV"
      },
      "outputs": [],
      "source": [
        "# There are many ways to define BOW: these are four of the most common ways\n",
        "\n",
        "[Xtrain, ytrain, Xtest, ytest] = create_test_train(all_sents,'count',1000)\n",
        "# [Xtrain, ytrain, Xtest, ytest] = create_test_train(1000, 'freq',first_label_df ,second_label_df)\n",
        "# [Xtrain, ytrain, Xtest, ytest] = create_test_train(1000, 'binary',first_label_df ,second_label_df)\n",
        "# [Xtrain, ytrain, Xtest, ytest] = create_test_train(1000, 'tfidf',first_label_df ,second_label_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4F5wF7D5BPV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeHkNZiO5BPZ"
      },
      "source": [
        "### Create models: MultiLayer Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylqYT5BC5BPZ",
        "outputId": "931c135a-1bc8-49f8-c44c-8e379b5d8650"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "2954/2954 - 3s - loss: 0.5127 - accuracy: 0.7478\n",
            "Epoch 2/50\n",
            "2954/2954 - 2s - loss: 0.4758 - accuracy: 0.7679\n",
            "Epoch 3/50\n",
            "2954/2954 - 2s - loss: 0.4597 - accuracy: 0.7789\n",
            "Epoch 4/50\n",
            "2954/2954 - 2s - loss: 0.4444 - accuracy: 0.7894\n",
            "Epoch 5/50\n",
            "2954/2954 - 2s - loss: 0.4281 - accuracy: 0.7999\n",
            "Epoch 6/50\n",
            "2954/2954 - 2s - loss: 0.4096 - accuracy: 0.8100\n",
            "Epoch 7/50\n",
            "2954/2954 - 2s - loss: 0.3898 - accuracy: 0.8223\n",
            "Epoch 8/50\n",
            "2954/2954 - 2s - loss: 0.3690 - accuracy: 0.8329\n",
            "Epoch 9/50\n",
            "2954/2954 - 2s - loss: 0.3476 - accuracy: 0.8446\n",
            "Epoch 10/50\n",
            "2954/2954 - 2s - loss: 0.3272 - accuracy: 0.8548\n",
            "Epoch 11/50\n",
            "2954/2954 - 2s - loss: 0.3087 - accuracy: 0.8635\n",
            "Epoch 12/50\n",
            "2954/2954 - 2s - loss: 0.2927 - accuracy: 0.8706\n",
            "Epoch 13/50\n",
            "2954/2954 - 2s - loss: 0.2784 - accuracy: 0.8768\n",
            "Epoch 14/50\n",
            "2954/2954 - 2s - loss: 0.2657 - accuracy: 0.8832\n",
            "Epoch 15/50\n",
            "2954/2954 - 2s - loss: 0.2549 - accuracy: 0.8876\n",
            "Epoch 16/50\n",
            "2954/2954 - 2s - loss: 0.2459 - accuracy: 0.8917\n",
            "Epoch 17/50\n",
            "2954/2954 - 2s - loss: 0.2372 - accuracy: 0.8964\n",
            "Epoch 18/50\n",
            "2954/2954 - 2s - loss: 0.2298 - accuracy: 0.8989\n",
            "Epoch 19/50\n",
            "2954/2954 - 2s - loss: 0.2233 - accuracy: 0.9014\n",
            "Epoch 20/50\n",
            "2954/2954 - 2s - loss: 0.2185 - accuracy: 0.9038\n",
            "Epoch 21/50\n",
            "2954/2954 - 2s - loss: 0.2132 - accuracy: 0.9051\n",
            "Epoch 22/50\n",
            "2954/2954 - 2s - loss: 0.2087 - accuracy: 0.9071\n",
            "Epoch 23/50\n",
            "2954/2954 - 2s - loss: 0.2050 - accuracy: 0.9090\n",
            "Epoch 24/50\n",
            "2954/2954 - 2s - loss: 0.2015 - accuracy: 0.9095\n",
            "Epoch 25/50\n",
            "2954/2954 - 2s - loss: 0.1980 - accuracy: 0.9120\n",
            "Epoch 26/50\n",
            "2954/2954 - 2s - loss: 0.1950 - accuracy: 0.9125\n",
            "Epoch 27/50\n",
            "2954/2954 - 2s - loss: 0.1918 - accuracy: 0.9143\n",
            "Epoch 28/50\n",
            "2954/2954 - 2s - loss: 0.1895 - accuracy: 0.9146\n",
            "Epoch 29/50\n",
            "2954/2954 - 2s - loss: 0.1872 - accuracy: 0.9147\n",
            "Epoch 30/50\n",
            "2954/2954 - 2s - loss: 0.1854 - accuracy: 0.9158\n",
            "Epoch 31/50\n",
            "2954/2954 - 2s - loss: 0.1835 - accuracy: 0.9172\n",
            "Epoch 32/50\n",
            "2954/2954 - 2s - loss: 0.1816 - accuracy: 0.9180\n",
            "Epoch 33/50\n",
            "2954/2954 - 2s - loss: 0.1796 - accuracy: 0.9187\n",
            "Epoch 34/50\n",
            "2954/2954 - 2s - loss: 0.1783 - accuracy: 0.9182\n",
            "Epoch 35/50\n",
            "2954/2954 - 2s - loss: 0.1765 - accuracy: 0.9196\n",
            "Epoch 36/50\n",
            "2954/2954 - 2s - loss: 0.1751 - accuracy: 0.9201\n",
            "Epoch 37/50\n",
            "2954/2954 - 2s - loss: 0.1737 - accuracy: 0.9207\n",
            "Epoch 38/50\n",
            "2954/2954 - 2s - loss: 0.1721 - accuracy: 0.9211\n",
            "Epoch 39/50\n",
            "2954/2954 - 2s - loss: 0.1712 - accuracy: 0.9209\n",
            "Epoch 40/50\n",
            "2954/2954 - 2s - loss: 0.1702 - accuracy: 0.9214\n",
            "Epoch 41/50\n",
            "2954/2954 - 2s - loss: 0.1690 - accuracy: 0.9216\n",
            "Epoch 42/50\n",
            "2954/2954 - 2s - loss: 0.1684 - accuracy: 0.9217\n",
            "Epoch 43/50\n",
            "2954/2954 - 2s - loss: 0.1672 - accuracy: 0.9229\n",
            "Epoch 44/50\n",
            "2954/2954 - 2s - loss: 0.1660 - accuracy: 0.9229\n",
            "Epoch 45/50\n",
            "2954/2954 - 2s - loss: 0.1655 - accuracy: 0.9228\n",
            "Epoch 46/50\n",
            "2954/2954 - 2s - loss: 0.1645 - accuracy: 0.9240\n",
            "Epoch 47/50\n",
            "2954/2954 - 2s - loss: 0.1636 - accuracy: 0.9232\n",
            "Epoch 48/50\n",
            "2954/2954 - 2s - loss: 0.1627 - accuracy: 0.9240\n",
            "Epoch 49/50\n",
            "2954/2954 - 2s - loss: 0.1620 - accuracy: 0.9246\n",
            "Epoch 50/50\n",
            "2954/2954 - 2s - loss: 0.1614 - accuracy: 0.9251\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x2abef9059bd0>"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define Network\n",
        "n_words = Xtrain.shape[1]\n",
        "model = Sequential()\n",
        "model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile network\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# fit network\n",
        "model.fit(Xtrain, ytrain, epochs=50, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35mZvxES5BPZ",
        "outputId": "9ef3c678-0361-4d18-c06e-8a368977b63b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 67.699999\n"
          ]
        }
      ],
      "source": [
        "# evaluate mode Freq\n",
        "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDuPHxOJ5BPZ",
        "outputId": "c41c5327-fb4f-4185-e5a2-426d7b4b8117"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 66.900003\n"
          ]
        }
      ],
      "source": [
        "# evaluate mode count\n",
        "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cszgDMQ75BPZ",
        "outputId": "09adf265-3d0e-47f2-efe1-1b2010411a59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 65.549999\n"
          ]
        }
      ],
      "source": [
        "# evaluate mode binary\n",
        "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwlEyXuY5BPZ",
        "outputId": "070713e9-a8c7-45ab-d01c-f6612e189628"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 64.899999\n"
          ]
        }
      ],
      "source": [
        "# evaluate mode tfidf\n",
        "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c71bcGAd5BPa"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QT0XM8A5BPa"
      },
      "outputs": [],
      "source": [
        "Xtrain, ytrain, Xtest, ytest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kF6nJIpD5BPa"
      },
      "outputs": [],
      "source": [
        "def simple_logistic_classify(Xtrain, ytrain, Xtest, ytest, description, _C=1.0):\n",
        "    model = LogisticRegression(C=_C).fit(Xtrain, ytrain)\n",
        "    score = model.score(Xtest, ytest)\n",
        "    print('Test Score with', description, 'features', score)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPXV7F-55BPa",
        "outputId": "dddc8b19-b487-4153-c930-506b83903327"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Score with frequency features 0.6615\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/cluster/tufts/sunterlab/coguik01/conda/condaenv/Emeka_conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        }
      ],
      "source": [
        "# evaluate mode Freq\n",
        "model = simple_logistic_classify(Xtrain, ytrain, Xtest, ytest, 'frequency', _C=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qaAiLnz5BPa",
        "outputId": "d2a928b1-7e6e-4e86-806b-54781b6b984f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Score with frequency features 0.6855\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/cluster/tufts/sunterlab/coguik01/conda/condaenv/Emeka_conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        }
      ],
      "source": [
        "# evaluate mode binary\n",
        "model = simple_logistic_classify(Xtrain, ytrain, Xtest, ytest, 'frequency', _C=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwipcPaj5BPa",
        "outputId": "a8801cbc-130a-4596-f7ac-439e8fd455c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Score with frequency features 0.689\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/cluster/tufts/sunterlab/coguik01/conda/condaenv/Emeka_conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        }
      ],
      "source": [
        "# evaluate mode count\n",
        "model = simple_logistic_classify(Xtrain, ytrain, Xtest, ytest, 'frequency', _C=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sq2EO0A5BPa",
        "outputId": "71f64657-e2cf-4a9c-f382-612e3f07ecb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Score with frequency features 0.6915\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/cluster/tufts/sunterlab/coguik01/conda/condaenv/Emeka_conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        }
      ],
      "source": [
        "# evaluate mode tfidf\n",
        "model = simple_logistic_classify(Xtrain, ytrain, Xtest, ytest, 'frequency', _C=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wg2Ou0iG5BPa"
      },
      "outputs": [],
      "source": [
        "# ideas gam; word2vec with words associaated with data in bow (include words associated with data to the verb features);"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Emeka_conda",
      "language": "python",
      "name": "emeka_conda"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}